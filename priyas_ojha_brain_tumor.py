# -*- coding: utf-8 -*-
"""Priyas Ojha project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lvrsqavv-tuLpcYGagzw6vRUfm76gfkr

# **Problem Statement**

Sources for current idea:
1.https://archive.ics.uci.edu/dataset/759/glioma%2Bgrading%2Bclinical%2Band%2Bmutation%2Bfeatures%2Bdataset?utm_source=chatgpt.com
2. https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor/data
3.

**Describe the business problem, providing context by explaining the industry and specific issue.**

Brain Tumors are life-threatening conditions that require timely and accurate diagnosis for effective treatments. In the healthcare industry, especially in medical imaging and diagnostics, detecting tumors involves manual analysis of MRI scans by radiologists. This is often slow and subject to human error. In low-resource areas, access to skilled radiologists is limited delaying diagnosis and treatment.
The issue here is the lack of fast, scalable, and accurate tumor detection methods that can support medical personnel in making decisions. Making an ML model to predict the presence of a tumor from extracted image features helps make this process faster.

**Highlight the value of solving it, like cost savings, increased revenue, and improved customer satisfaction, and identify stakeholders who will benefit, including the company, customers, and employees**

Cost Savings: Automating part of the diagnostic workflow reduces the need for multiple manual reviews and minimizes unnecessary tests caused by false positive

Improved Patient OutcomesL Faster more accurate detection means earlier intervention, which dramatically improves survival rates and reduces complications.

Increased Efficiency: Hospitals and labs can process more patients in less time, optimizing the use of limited medical personnel and equipment.

Risk Reduction: Reducing false negatives helps prevent missed diseases which can be medically costly

Patients would receive faster and more accurate diagnoses improving health outcomes.

Doctors would have a reduced workload.

Hospitals would improve service efficiency, and their reputation, and reduce costs associated with delayed or incorrect diagnoses.

**Outline potential machine learning methods, such as regression, classification, and clustering, and justify their suitability.**

Given the task, predicting whether a brain scan shows a tumor or not, the most appropriate ML method is classification. Specifically, this is a binary classification problem because the target variable has two possible outcomes: tumor 1 or no tumor 0.

ML methods:

SVM. Effective for small to medium-sized datasets with clear margins of separation.

K-nearest Neighbors (KNN). Simple and interpretable, useful when data is well distributed.

These methods are appropriate because the dataset is tabular, numerical, and well-structured. Each instance represents a brain scan already summarised into quantitative features, so traditional classifiers can work effectively without the need for image processing.

**Discuss data-related challenges, like quality and availability, and modeling risks, such as overfitting and bias, as well as deployment challenges and ensuring the modelâ€™s business usefulness.**

Since the dataset is already preprocessed and structured with extracted features, there are several challenges and risks to consider.

Data Quality and Availability: Class imbalance: If there are significantly more non tumor cases than tumor cases, the model can be biased twords predicting the majority. Noise: Some features may contain redundant or noisy data that can mislead the model. ANOVA or PCA will be necessary. Limited sample size: If the dataset is small it increases the risk of overfitting, where the model preforms well on training data but poorly on unseen data.

Modeling Risks:
Overfitting: With high dimentional data and small datasets, models may memorise rather than genralise, cross validation can help midigate this like k fold cross. Bias: If the data collection process was skwed the model may not generalise well in the real world.

Deployment Challenges:
Genalibity: The model may not perform as well on MRI data from other hospitals if the equipment isn't standardized. Clinical Integration: For real-world use, the model needs to be validated with clinical trains and approved. Update and retraining: Over time new image processing and changes in data distributions might require the model to be retrained to keep up with performance

# **Exploratory Data Analysis**

## Data Description

**Source**: Mention where you obtained the data and its format.

The project uses the Brain Tumor dataset from Kaggle, which includes statistical features derived from MRI images to classify the presence of a brain tumor (1 = Tumor, 0 = Non-Tumor).  https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor/data

**Description**: Provide a detailed description of the dataset including the number of samples, features, and any relevant metadata such as the column names and data types.

The dataset contains 3,762 samples and 15 columns, including 13 statistical features, 1 image identifier, and 1 binary target column. All features are numeric except for the Image column. There are no missing values.
"""

!pip install kagglehub --quiet

import kagglehub

# Download the latest version of the brain tumor dataset
path = kagglehub.dataset_download("jakeshbohaju/brain-tumor")

print("Path to dataset files:", path)

# Step 3: Load the dataset into pandas
import pandas as pd
import os

# Locate the CSV file inside the downloaded folder
csv_path = os.path.join(path, "Brain Tumor.csv")
brain_df = pd.read_csv(csv_path)

print("Citation:\n")
print("Jakesh Bohaju. (2023). Brain Tumor Dataset. Kaggle. https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor")

print("Check out this dataset on Kaggle for more information:")
print("https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor")

print(" VVV The first 5 rows of the data set VVV \n")
print(brain_df.head())
print("---------------")
print(" VVV Number of rows and columns VVV \n")
print(brain_df.shape)
print("---------------")
print(" VVV Data type of each column VVV \n")
print(brain_df.dtypes)
print("---------------")
print(" VVV Number of null values (or not-null values) in each column VVV \n")
print(brain_df.isnull().sum())
print("---------------")
print(" VVV General statistics of each numeric column (min, max, mean, standard deviation) VVV \n")
print(brain_df.describe().loc[['min', 'max', 'mean', 'std']])

"""## Exploratory Analysis

**Data Cleaning:** Describe any data cleaning steps taken (e.g., handling missing values, outlier treatment, reformatting).

Removed Non-Predictive Column: The Image column was dropped, as it is an identifier and does not contain any useful information for modeling.

Checked for Missing Values: Verified that there are no missing values in any column. No imputation or row removal was necessary.

Assessed Feature Variance: The Coarseness colum seemed constant based on summary statistics, but a check showed it has 146 unique values. So it was stayed for modeling, as it carries variability and may contribute to prediction.
"""

brain_df_cleaned = brain_df.drop(columns=['Image'])

# Keep Coarseness
print("Unique values in 'Coarseness':", brain_df_cleaned['Coarseness'].nunique())

# Confirm final cleaned shape
print("Cleaned dataset shape:", brain_df_cleaned.shape)

"""**Summary Statistics:** Provide summary statistics of the data (e.g., mean, median, standard deviation). Note any important findings from the summary statistics."""

print(" VVV The first 5 rows of the data set VVV \n")
print(brain_df_cleaned.head())
print("---------------")
print(" VVV Number of rows and columns VVV \n")
print(brain_df_cleaned.shape)
print("---------------")
print(" VVV Data type of each column VVV \n")
print(brain_df_cleaned.dtypes)
print("---------------")
print(" VVV Number of null values (or not-null values) in each column VVV \n")
print(brain_df_cleaned.isnull().sum())
print("---------------")
print(" VVV General statistics of each numeric column (min, max, mean, standard deviation) VVV \n")
print(brain_df_cleaned.describe().loc[['min', 'max', 'mean', 'std']])

"""Visualizations: Include various visualizations to understand data distribution and relationships between variables (e.g., histograms, scatter plots, box plots)."""

import matplotlib.pyplot as plt
import seaborn as sns

#histogram
brain_df_cleaned.hist(bins=30, figsize=(16, 12), edgecolor='black')
plt.suptitle("Feature Distributions", fontsize=18)
plt.tight_layout()
plt.show()

# Box plots to compare feature distributions by class
plt.figure(figsize=(18, 14))
i = 1  # subplot index starts at 1, we skip class because thats what we want to see
for col in brain_df_cleaned.columns[1:]:  # skip 'Class'
    plt.subplot(4, 4, i)
    sns.boxplot(data=brain_df_cleaned, x="Class", y=col)
    plt.title(f"{col} vs. Tumor Class")
    i +=1

plt.tight_layout()
plt.show()

"""**Insights**: Discuss the key insights and patterns discovered during EDA that will be relevant to the machine learning model.

The Summary Statistics and Visualizations showed several important patterns. Features like Entropy, Energy, and ASM had skewed distributions, with tumor cases generally showing lower values. Boxplots highlighted that tumor (Class 1) and non-tumor (Class 0) samples differed significantly across several variables such as in features like Contrast, Homogeneity, and Dissimilarity. Overall, the visual trends support the idea that a classification model should be able to learn from these patterns to distinguish between tumor and non-tumor images.

## Statistical Analysis

**Correlation Analysis**: Examine the relationships between different variables using correlation coefficients and scatter plots.
"""

plt.figure(figsize=(12, 10))
sns.heatmap(brain_df_cleaned.corr(), annot=True, cmap='coolwarm', fmt='.2f') #ice fire doesnt look as good on this
plt.title("Feature Correlation Heatmap")
plt.show()

feature_pairs = [
    ('Energy', 'ASM'),
    ('Entropy', 'Homogeneity'),
    ('Contrast', 'Dissimilarity')
] #just chose a couple to do scatter plot since heatmap is shown above

# Set up the plot
plt.figure(figsize=(15, 12))

# Manual index for subplots
i = 1
for pair in feature_pairs:
    x_feat, y_feat = pair
    plt.subplot(2, 2, i)
    sns.scatterplot(data=brain_df_cleaned, x=x_feat, y=y_feat, hue="Class", alpha=0.7, palette="icefire")
    plt.title(f'{x_feat} vs {y_feat} by Class')
    plt.xlabel(x_feat)
    plt.ylabel(y_feat)
    i += 1

plt.tight_layout()
plt.show()

"""A heatmap was used to look at how different features are related. Some features like Entropy, Energy, and ASM had very high correlations with each other, meaning they carry similar information. Skewness and Kurtosis were also closely related. Scatter plots helped confirm this by showing strong patterns between those feature pairs. These relationships suggest that some features may be repetitive, and we might be able to reduce them using techniques like PCA.

**Hypothesis Testing:** Conduct any relevant hypothesis tests between important features that are suspected to have a key relationship to determine if that relationship can or should be learned by the model.
"""

from sklearn.feature_selection import f_classif
import pandas as pd

# Separate features and target from full cleaned dataset
X = brain_df_cleaned.drop('Class', axis=1)
y = brain_df_cleaned['Class']

# Perform ANOVA F-test
f_values, p_values = f_classif(X, y)

# Create a DataFrame of results
anova_results = pd.DataFrame({
    'Feature': X.columns,
    'F-Value': f_values,
    'p-Value': p_values
}).sort_values(by='F-Value', ascending=False)

# Show results
print("ANOVA F-scores for features:")
print(anova_results)

#grab top 4 features
top_4_features = anova_results.head(4)['Feature'].tolist()
print("Top 3 features selected via ANOVA:", top_4_features)

"""An ANOVA F-test was used to identify which features have the most significant differences between tumor and non-tumor groups. Energy, Homogeneity, Entropy, and ASM had the highest F-values and extremely low p-values, showing that they are the most useful features for separating the two classes. These results confirm earlier findings from the correlation matrix and boxplots.

# **Models**

## Feature Engineering & Selection

Since the dataset consisted of numerical features, no categorical encoding was needed. Also did not need to create new features because the original dataset consisted of well-defined numerical features extracted from the brain MRI image. Such as Entropy, Enery, ASM, etc ...

For feature selection, used a filter method based on the ANOVA F-Test, which evaluated the statistical significance of each feature in relation to the target variable (class so tumor or not). This method calculated the F-Valyes and P-values to identify which features best separated the tumor and non tumor classes.

Based on the results from the ANOVA scores **Energy**, **Homogeneity**, **Entropy**, and **ASM** had the highest F-scores and extremely low P-values of basically 0. They were also visually confirmed to show a clear separation between classes earlier in the box plots.

## Model Tuning, Validation, and Evaluation
"""

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

print("Using top 4 features:", top_4_features)

# did this so it keeps the top 4 even when data changes and new tops come out
X = brain_df_cleaned[top_4_features]
y = brain_df_cleaned['Class']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

classifiers = {
    'KNN': KNeighborsClassifier(),
    'MLP': MLPClassifier(),
    'SVC': SVC()
}

# Evaluate models with 10-fold cross-validation (F1-score)
cv_results = {}
print("10-Fold Cross-Validation F1 Scores:\n")

for name, classifier in classifiers.items():
    scores = cross_val_score(classifier, X_scaled, y, cv=10, scoring='f1')
    cv_results[name] = scores.mean()
    print(f"{name} F1-score: {scores.mean():.4f} (std: {scores.std():.4f})")

# Split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Retrain the best model on the full training set
final_model = MLPClassifier()
final_model.fit(X_train_scaled, y_train)

# Predict on test set
y_pred = final_model.predict(X_test_scaled)


print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("Classification Report:")
print(classification_report(y_test, y_pred))


print("Number of non tumors and tumors")
print(brain_df_cleaned['Class'].value_counts())

"""***List the machine learning models you considered and why they were chosen***

Considered three machine learning models for this classification task:

* K-Nearest Neighbors (KNN): chosen because of its simplicity and strong preformance on small datasets
* Support Vector Classifier (SVC): chosen because of its effectiveness in binary classification and its ability to handle dimensional data
* Multi-Layer Perceptron (MLP): used for its ability to model nonlinear relationships

***Explain the process of training the models, including any considerations for handling imbalanced or sparse data if applicable. Also describe how you validated the models, e.g., k-fold cross-validation***

The dataset was already pretty balanced between non-tumor and tumor cases (2079,1683) , so no class rebalancing techniques were required. Used the top 4 features selected through ANOVA to train each model.

To ensure validation, used 10-fold cross-validation for all three models. This method splits the dataset into 10 parts, trains the model on 9 of them, and validates the remaining part. The process is repeated 10 times, and the average score is used to assess performance. This helped minimize overfitting and made sure the model generalized the data well.

***List the performance metrics used to evaluate the models (e.g., accuracy, precision, recall, F1-score, ROC-AUC). Compare the performance of different models and justify your final model selection.***

Used the following evaluation metrics:
* F1-Score: chosen as the main metric because of its balance between precision and recall
* Precision: useful to measure how many predicted tumors were right
* Recall: important to check how many actual tumors were found
* Accuracy: to give an overall view of the model performance

***Compare the performance of different models and justify your final model selection.***

All three models preformed very well, with F1-Scores above 0.97


* KNN F1-score: 0.9741 (std: 0.0078)
* MLP F1-score: 0.9759 (std: 0.0092)
* SVC F1-score: 0.9734 (std: 0.0090)



Although the preformance differences were small, MLP classifer had the highest average f1-score. It also preformed well on the final test set with 97% accuracy and a strong precision/recall. Making MLP a reliable choice for the final model

# **Conclusion**

In this project, I built a machine-learning model to classify brain tumor presence based on numerical features from MRI images. After cleaning the data and conducting explotorary analysis I selected the top 4 most predictive features (Energy, Honogenity, Entropy, and ASM) using an ANOVA F-test. I tested three different ML models (KNN, SVC<, and MLP) using 10-fold cross-validation. The MLP model had the highest average F1 score and was selected for final testing. When evaluated on a separate test set the final MLP model had a 97% accuracy with strong precision and recall, showing that the selected features and modeling were effective in solving the problem.

## Key Findings

During EDA, I found that serveral features (Energy, Entropy, Homogenity, and ASM) showed clear sepration between tumor and non tumor cases. Tumor samples generaly had lower values in features like Entropy and Energy, while non tumor samples had higher more consistent distributions.

Statistical analysis using ANOVA confirmed these observations with the top 4 features habing the hightest F-values and lowest p-values, indicating strong differences between the two classes.

In model testing, all three models performed well with f1 scores above 0.97. The final selected model MLP had a 97% accuracy on the test along with high precision and recall for both tumor and nontumor detection. This confirmed that the selected features and model were highly effective for the classification.

## Problem resolution

The problem of accurately classifying brain tumor presence based on extracted image features was efficiently solved by the final ML model. The final MLP having 97% accuracy with high precision and recall suggests the model is reliable at detecting both tumor and nontumor cases, minimizing both false neg and false pos which is crucial in medical settings.

The potential business impact of this is significant. A model like this could be used to assist radiologists by providing a second opinion during diagnosis, helping to speed up the review process, reducing human error, and improving early detection.

## Deployment

The model can be used in hospitals to help doctors by giving a second opinion after an MRI scan.

New predictions would happen every time a new MRI scan is done, the model can run automatically in the background and give a quick result.

Radiologists would still review the models predictiom before making any final decisions because the model is to help not replace doctors.

I would expect the model to need re trainign every year or so because new types of data and updated medical tech could change how tumors appear.

## Improvements

If I continued the project, I would try a few improvements.

First I would collect more MRI scans to train the model on a larger and more diverse dataset. More data could help the model generalize better to new patients.

I would also try other machine learning models like Random Forest or Gradient Boosting to see if they could outperform MLP.
"""